{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping through reddit's API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw as praw\n",
    "import pandas as pd\n",
    "\n",
    "client_id = 'mr1N30-RtWkrwHFIVwrcuQ'\n",
    "client_secret = 'ansKPya9-TJl19H1Suz3EUjKzpMygw'\n",
    "user_agent = 'DURI Research'\n",
    "products = {\n",
    "    'Electric Skateboard',\n",
    "    'Electric Scooters',\n",
    "    'Scooter',\n",
    "    'Skateboard'\n",
    "}\n",
    "\n",
    "dict_values = dict()\n",
    "injury_words = {\n",
    "    'injury',\n",
    "    'accident',\n",
    "    'crashed',\n",
    "    'crash',\n",
    "}\n",
    "\n",
    "reddit_obj = praw.Reddit(client_id=client_id, client_secret=client_secret, user_agent=user_agent)\n",
    "\n",
    "subreddit = reddit_obj.subreddit(\"all\")\n",
    "i = 0\n",
    "data = []\n",
    "query = \"selftext:[skateboard injury]\"\n",
    "print(query)\n",
    "for submission in subreddit.search(query, limit=None):\n",
    "    if submission.is_self:\n",
    "        text = submission.selftext\n",
    "        words = text.split(\" \")\n",
    "        if len(words) > 5:\n",
    "            data.append(text)\n",
    "            i += 1\n",
    "print(i)\n",
    "\n",
    "\n",
    "'''\n",
    "for product in products:\n",
    "    for word in injury_words:\n",
    "        query = \"selftext:[skateboard injury]\"\n",
    "        print(query)\n",
    "        for submission in subreddit.search(query, limit=None):\n",
    "            if submission.is_self:\n",
    "                text = submission.selftext\n",
    "                words = text.split(\" \")\n",
    "                if len(words) > 5:\n",
    "                    data.append(text)\n",
    "                    i += 1\n",
    "        print(i)\n",
    "'''\n",
    "data = set(data)\n",
    "print(len(data))\n",
    "\n",
    "dataframe = pd.DataFrame(data, columns=[\"text\"])\n",
    "dataframe.to_csv('/dataFiles/redditAPIScrape.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selenium Scrape using Raw HTML Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import requests\n",
    "\n",
    "text_list = []\n",
    "links = []\n",
    "q = \"https://www.reddit.com/search/?q=skateboard%20accident\"\n",
    "driver = webdriver.Chrome(executable_path=\"/Users/harmyabhatt/Downloads/chromedriver\")\n",
    "\n",
    "driver.get(f\"{q}\")\n",
    "time.sleep(3)\n",
    "\n",
    "prev = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "    driver.execute_script(\"window.scrollBy(0, document.body.scrollHeight);\")\n",
    "    time.sleep(3)\n",
    "    new = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if i == 4:  # change number to get more data, this signifies the number of times the page will be scrolled till the end\n",
    "        elements = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "        for element in elements:\n",
    "            link = element.get_attribute(\"href\")\n",
    "            if link.__contains__(\"/comments\"):\n",
    "                links.append(link)\n",
    "        break\n",
    "    prev = new\n",
    "    i += 1\n",
    "\n",
    "print(len(links))\n",
    "print(links)\n",
    "\n",
    "\n",
    "narratives = []\n",
    "text = []\n",
    "for link in links:\n",
    "    driver.get(link)\n",
    "    post = driver.find_element(By.CSS_SELECTOR, \"[data-test-id=post-content]\")\n",
    "    text = post.text.strip().split('\\n')\n",
    "    post_text = \"\"\n",
    "    for t in text:\n",
    "        if len(t.split(' ')) > 10:\n",
    "            narratives.append(t)\n",
    "    paragraphs = driver.find_elements(By.CSS_SELECTOR, \"[data-testid=comment]\")\n",
    "    for paragraph in paragraphs:\n",
    "        data = paragraph.text\n",
    "        narratives.append(data)\n",
    "\n",
    "    print(\"GOO\")\n",
    "    time.sleep(5)\n",
    "\n",
    "print(narratives)\n",
    "print(\"Fin.\")\n",
    "df = pd.DataFrame()\n",
    "df['text'] = narratives\n",
    "df.to_csv('/dataFiles/redditRawScrape.csv', index=False)\n",
    "print(narratives)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
